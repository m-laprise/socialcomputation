{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_densematrix(dataset_size: int, m: int, n: int, *, \n",
    "                         seed: int, measurement_type: str = \"mask\",\n",
    "                         include_X: bool = False, mask_prob: float = 0.33, \n",
    "                         rank_threshold: float = 0.1) -> Tuple[torch.Tensor, torch.Tensor, \n",
    "                                                               torch.Tensor, torch.Tensor]:\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Generate matrix ranks\n",
    "    rank = torch.randint(1, min(m, n) // 2, (dataset_size,))\n",
    "    n_seen = round((1 - mask_prob) * m * n)\n",
    "    rank = torch.minimum(rank, torch.tensor(n_seen // 2))\n",
    "    \n",
    "    # Generate low-rank matrices by multiplying m x r and r x n matrices\n",
    "    A = torch.randn(dataset_size, m, rank.max().item())\n",
    "    B = torch.randn(dataset_size, rank.max().item(), n)\n",
    "    \n",
    "    # Create a mask to zero out extra columns in A and rows in B\n",
    "    mask_A = torch.arange(rank.max().item()).expand(dataset_size, -1) < rank.unsqueeze(1)\n",
    "    mask_B = torch.arange(rank.max().item()).expand(dataset_size, -1) < rank.unsqueeze(1)\n",
    "    \n",
    "    A = A * mask_A.unsqueeze(1)\n",
    "    B = B * mask_B.unsqueeze(2)\n",
    "    \n",
    "    X = torch.matmul(A, B)\n",
    "    \n",
    "    # Generate labels with shape (dataset_size, 1)\n",
    "    Y = (rank < min(m, n) * rank_threshold).int().unsqueeze(1)\n",
    "    \n",
    "    if measurement_type == \"mask\":\n",
    "        # Generate measurements of each X by masking some of the entries; dataset_size x m x n\n",
    "        masks = torch.bernoulli(torch.full((dataset_size, m, n), 1 - mask_prob))\n",
    "        Xhat = X * masks\n",
    "    \n",
    "    elif measurement_type == \"trace\":\n",
    "        # Generate measurements of each X by taking Trace(X * O_i) for (2*maxrank) random matrices O_i\n",
    "        O = torch.randn(dataset_size, 250, 50, m)\n",
    "        # Function to compute the trace of each product \n",
    "        def trace_product(X, O):\n",
    "            product = torch.matmul(O, X.unsqueeze(-1)).squeeze(-1)\n",
    "            trace = torch.diagonal(product, dim1=-2, dim2=-1).sum(-1)\n",
    "            return trace\n",
    "        vectorized_trace_product = torch.vmap(\n",
    "            torch.vmap(trace_product, in_dims=(None, 0)), \n",
    "            in_dims=(0, 0))\n",
    "        # Compute the traces and products\n",
    "        Xhat = vectorized_trace_product(X, O)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Measurement type not recognized. Choose from 'mask' or 'trace'.\")\n",
    "    \n",
    "    if include_X:\n",
    "        return X, Xhat, Y, rank\n",
    "    else:\n",
    "        return Xhat, Y, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 25, 25]),\n",
       " torch.Size([10, 25, 25]),\n",
       " torch.Size([10, 250]),\n",
       " torch.Size([10, 1]),\n",
       " torch.Size([10]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test matrix data generation and visualize the data samples\n",
    "dataset_size = 10\n",
    "X, Xhat, Y, rank = get_data_densematrix(dataset_size, 25, 25, seed = 12,\n",
    "                                        measurement_type=\"mask\", include_X=True)\n",
    "traces, Y, rank = get_data_densematrix(dataset_size, 25, 25, seed = 12,\n",
    "                                       measurement_type=\"trace\", include_X=False)\n",
    "X.shape, Xhat.shape, traces.shape, Y.shape, rank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(3, 4, figsize=(10, 10))\n",
    "for i in range(4):\n",
    "    axs[0, i].imshow(X[i], cmap=\"gray\")\n",
    "    axs[1, i].imshow(Xhat[i], cmap=\"gray\")\n",
    "    # Take traces vector and stack copies of the same trace to visualize\n",
    "    axs[2, i].imshow(traces[i] * np.ones((traces.shape[-1], traces.shape[-1])), cmap=\"gray\")\n",
    "    axs[0, i].set_title(f\"Ground truth X[{i}], r={np.linalg.matrix_rank(X[i])}\")\n",
    "    axs[1, i].set_title(f\"Mask Xhat1[{i}]\")\n",
    "    axs[2, i].set_title(f\"Traces Xhat2[{i}]'\")\n",
    "plt.suptitle(\"DGP - Ground truth and measurement samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, Xhat, Y, rank, X=None):\n",
    "        self.Xhat = Xhat\n",
    "        self.Y = Y\n",
    "        self.rank = rank\n",
    "        self.X = X\n",
    "        self.include_X = X is not None\n",
    "        self.dataset_size = Xhat.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.include_X:\n",
    "            return self.X[idx], self.Xhat[idx], self.Y[idx], self.rank[idx]\n",
    "        else:\n",
    "            return self.Xhat[idx], self.Y[idx], self.rank[idx]\n",
    "\n",
    "# Parameters for data generation\n",
    "dataset_size = 1000\n",
    "m = 250\n",
    "n = 250\n",
    "seed = 42\n",
    "measurement_type = \"mask\"\n",
    "include_X = True\n",
    "mask_prob = 0.33\n",
    "rank_threshold = 0.1\n",
    "\n",
    "# Generate data\n",
    "data = get_data_densematrix(dataset_size, m, n, \n",
    "                            seed=seed, \n",
    "                            measurement_type=measurement_type, \n",
    "                            include_X=include_X, \n",
    "                            mask_prob=mask_prob, \n",
    "                            rank_threshold=rank_threshold)\n",
    "\n",
    "# Unpack generated data\n",
    "if include_X:\n",
    "    X, Xhat, Y, rank = data\n",
    "else:\n",
    "    Xhat, Y, rank = data\n",
    "    X = None\n",
    "\n",
    "# Create dataset\n",
    "dataset = CustomDataset(Xhat, Y, rank, X)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Iterate over DataLoader\n",
    "# for batch in dataloader:\n",
    "#    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is the mGRU from Kamesh Krishnamurthy\n",
    "# h(t+1) = \\sigma(Wh * h + Wx*x + b1) \\odot \\phi(Jh*h + Jx*x + b2)\n",
    "class mGRU(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        input_transform=None,\n",
    "        output_transform=None,\n",
    "        binaryoutput=False,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super(mGRU, self).__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.Wz = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "        self.Wh = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        torch.nn.init.xavier_normal_(self.Wz.weight)\n",
    "        torch.nn.init.xavier_normal_(self.Wh.weight, gain=5.0 / 3.0)\n",
    "        self.params = [self.Wh.weight, self.Wh.bias, \n",
    "                       self.Wz.weight, self.Wz.bias]\n",
    "        self.input_transform = input_transform\n",
    "        self.output_transform = output_transform\n",
    "\n",
    "        # learnable initial hidden state\n",
    "        self.initial_hidden_state = nn.Parameter(torch.zeros(1, hidden_size) * 0.05)\n",
    "        self.device = device\n",
    "        self.binaryoutput = binaryoutput\n",
    "        \n",
    "        if self.binaryoutput:\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "            torch.nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "    # hidden : batch_size x hidden_dim\n",
    "    # x : batch_size x seq_length x input_dim\n",
    "    def forward(self, x, hidden=None):\n",
    "        # hidden should be batch_size x hidden_dim\n",
    "        # x should be batch_size x seq_length x input_dim\n",
    "        if self.input_transform is not None:\n",
    "            x = self.input_transform(x)\n",
    "\n",
    "        _batch_size = x.shape[0]\n",
    "        _seq_len = x.shape[1]\n",
    "        _input_dim = x.shape[2]\n",
    "        _hidden_dim = self.hidden_size\n",
    "\n",
    "        if hidden is None:\n",
    "            # hidden = torch.zeros(1, _hidden_dim).repeat(_batch_size,1)\n",
    "            hidden = self.get_initial_state(_batch_size)\n",
    "        _output_seq = torch.zeros(_batch_size, _seq_len, _hidden_dim).to(self.device)\n",
    "\n",
    "        for t in range(x.shape[1]):\n",
    "            _ip = x[:, t, :]  # shape is batch_size x input_dim\n",
    "            # ip_combined = torch.cat((hidden[0,:,:],_ip),1)  \n",
    "            # # shape is batch_size x (input_dim + hidden_dim)\n",
    "            ip_combined = torch.cat((hidden, _ip), -1)  \n",
    "            # shape is batch_size x (input_dim + hidden_dim)\n",
    "            self.z = self.sigmoid(self.Wz(ip_combined))\n",
    "            hidden = torch.mul(self.z, self.tanh(self.Wh(ip_combined)))\n",
    "            _output_seq[:, t, :] = hidden\n",
    "\n",
    "        if self.output_transform is not None:\n",
    "            _output_seq = self.output_transform(_output_seq)\n",
    "\n",
    "        if self.binaryoutput:\n",
    "            # Apply the final linear layer and sigmoid activation for binary prediction\n",
    "            _output_seq = self.sigmoid(self.fc(_output_seq[:, -1, :]))\n",
    "            return _output_seq.squeeze(), hidden\n",
    "            \n",
    "        return _output_seq, hidden\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        return self.initial_hidden_state.repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialDecayLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, initial_lr, decay_rate, decay_steps, last_epoch=-1):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        super(ExponentialDecayLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.initial_lr * self.decay_rate ** (self.last_epoch / self.decay_steps) \n",
    "                for _ in self.base_lrs]\n",
    "\n",
    "# Example usage\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "#scheduler = ExponentialDecayLR(optimizer, initial_lr=0.1, decay_rate=0.96, decay_steps=100)\n",
    "#for epoch in range(1000):\n",
    "#    train(...)\n",
    "#    validate(...)\n",
    "#    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(train_loss, val_loss):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_loss, label=\"Training Loss\")\n",
    "    ax.plot(val_loss, label=\"Validation Loss\")\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Training and Validation Loss\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_eigenvalues(wR, multilayer: bool = False):\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.linspace(-1, 1, 1000)\n",
    "    ax.plot(x, np.sqrt(1 - x**2), 'k')\n",
    "    ax.plot(x, -np.sqrt(1 - x**2), 'k')\n",
    "    if multilayer:\n",
    "        for i, weights in enumerate(wR):\n",
    "            eigvals, _ = np.linalg.eig(weights)\n",
    "            ax.plot(np.real(eigvals), np.imag(eigvals), '.', label=f\"Layer {i}\")\n",
    "        ax.legend()\n",
    "    else:\n",
    "        eigvals, _ = np.linalg.eig(wR)\n",
    "        ax.plot(np.real(eigvals), np.imag(eigvals), '.')\n",
    "    ax.set_title(\"Eigenvalues of Recurrent Weights\")\n",
    "    ax.set_xlabel(\"Real\")\n",
    "    ax.set_ylabel(\"Imaginary\")\n",
    "    # equal axis\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    plt.show()    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mGRU(\n",
       "  (sigmoid): Sigmoid()\n",
       "  (tanh): Tanh()\n",
       "  (Wz): Linear(in_features=314, out_features=64, bias=True)\n",
       "  (Wh): Linear(in_features=314, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mGRU(input_size=250, hidden_size=64, device=mps_device)\n",
    "model.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=5000,\n",
    "    val_size=500,\n",
    "    batch_size=32,\n",
    "    initial_lr=3e-3,\n",
    "    decay_rate=0.999,\n",
    "    steps=500,\n",
    "    hidden_size=32,\n",
    "    depth=1,\n",
    "    seed=5678,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Generate data\n",
    "    xs, ys, _ = get_data_densematrix(dataset_size + val_size, 250, 250, \n",
    "                                     seed=seed, \n",
    "                                     mask_prob=1/2, \n",
    "                                     rank_threshold=0.2, \n",
    "                                     measurement_type=\"mask\", \n",
    "                                     include_X=False)\n",
    "    xs_train, ys_train = xs[:dataset_size], ys[:dataset_size]\n",
    "    xs_val, ys_val = xs[dataset_size:], ys[dataset_size:]\n",
    "    print(\"Generated training and validation data.\")\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(xs_train, ys_train)\n",
    "    val_dataset = TensorDataset(xs_val, ys_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                              shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                            shuffle=False, num_workers=4)\n",
    "    print(\"Created DataLoaders.\")\n",
    "    \n",
    "    # Create the model\n",
    "    inputdim = xs_train.shape[-1]\n",
    "    model = mGRU(input_size=inputdim, hidden_size=hidden_size, \n",
    "                 binaryoutput=True,\n",
    "                 device=device).to(device)\n",
    "    print(\"mGRU model initialized and sent to device.\")\n",
    "    \n",
    "    # Save the initial recurrent weights\n",
    "    initWz = model.Wz.weight.clone().detach().cpu().numpy()\n",
    "    print(\"Initial recurrent weights saved.\")\n",
    "    \n",
    "    # Define loss function (binary cross entropy) and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    print(\"Adam optimizer initialized.\")\n",
    "    \n",
    "    # Create learning rate schedule\n",
    "    scheduler = ExponentialDecayLR(optimizer, \n",
    "                                   initial_lr=initial_lr, \n",
    "                                   decay_rate=decay_rate, \n",
    "                                   decay_steps=steps)\n",
    "    print(\"Learning rate schedule created.\")\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    print(\"Training starting.\")\n",
    "    for step in range(steps):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, hidden = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch.float().squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs, hidden = model(x_batch)\n",
    "                val_loss += criterion(outputs, y_batch.float().squeeze()).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Append the training and validation loss\n",
    "        losses.append(loss.item())\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Print the loss every 50 steps, and at the final step\n",
    "        if step % 50 == 0 or step == steps - 1:\n",
    "            print(f\"step={step}, loss={np.round(loss.item(),6)}, val_loss={np.round(val_loss,6)}\")\n",
    "        \n",
    "        # Middle of training, save the recurrent weights\n",
    "        if step == steps // 2:\n",
    "            midWz = model.Wz.weight.clone().detach().cpu().numpy()\n",
    "    \n",
    "    endWz = model.Wz.weight.clone().detach().cpu().numpy()\n",
    "    \n",
    "    # Final accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_ys = model(xs_train.to(device))\n",
    "        num_correct = ((pred_ys > 0.5) == ys_train.to(device)).sum().item()\n",
    "        final_accuracy = num_correct / dataset_size\n",
    "    print(f\"Final accuracy = {np.round(final_accuracy,6)}\")\n",
    "    \n",
    "    # Plot training and validation loss curves\n",
    "    lossfig = plot_loss_curves(losses, val_losses)\n",
    "    \n",
    "    # Save the model\n",
    "    return model, lossfig, initWz, midWz, endWz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated training and validation data.\n",
      "Created DataLoaders.\n",
      "mGRU model initialized and sent to device.\n",
      "Initial recurrent weights saved.\n",
      "Adam optimizer initialized.\n",
      "Learning rate schedule created.\n",
      "Training starting.\n",
      "step=0, loss=0.738697, val_loss=0.709023\n",
      "step=9, loss=0.015143, val_loss=0.938615\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'tuple' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, lossfig, initWz, midWz, endWz \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmps_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 102\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(dataset_size, val_size, batch_size, initial_lr, decay_rate, steps, hidden_size, depth, seed, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    101\u001b[0m     pred_ys \u001b[38;5;241m=\u001b[39m model(xs_train\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m--> 102\u001b[0m     num_correct \u001b[38;5;241m=\u001b[39m ((\u001b[43mpred_ys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m) \u001b[38;5;241m==\u001b[39m ys_train\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    103\u001b[0m     final_accuracy \u001b[38;5;241m=\u001b[39m num_correct \u001b[38;5;241m/\u001b[39m dataset_size\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mround(final_accuracy,\u001b[38;5;241m6\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'tuple' and 'float'"
     ]
    }
   ],
   "source": [
    "model, lossfig, initWz, midWz, endWz = main(dataset_size=1000,\n",
    "    val_size=100,\n",
    "    batch_size=16,\n",
    "    steps=10,\n",
    "    device=mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initWz.shape, midWz.shape, endWz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 x 3 panel of plots showing the eigenvalues of the reset, input, and new weights, \n",
    "# at initialization, middle of training, and end of training.\n",
    "fig, axs = plt.subplots(3, 3, figsize=(13, 13))\n",
    "rowlabels = [\"Initialization\", \"Middle of Training\", \"End of Training\"]\n",
    "x = np.linspace(-1, 1, 1000)\n",
    "for i, (reset, inp, new) in enumerate(zip(resetW, inputW, newW)):\n",
    "    for j, weights in enumerate([reset, inp, new]):\n",
    "        axs[i, j].plot(x, np.sqrt(1 - x**2), 'k')\n",
    "        axs[i, j].plot(x, -np.sqrt(1 - x**2), 'k')\n",
    "        eigvals, _ = np.linalg.eig(weights)\n",
    "        axs[i, j].plot(np.real(eigvals), np.imag(eigvals), '.')\n",
    "        axs[i, j].set_title(f\"{rowlabels[i]}, {['Reset gate', 'Input gate', 'New hidden state'][j]}\")\n",
    "        axs[i, j].set_xlabel(\"Real\")\n",
    "        axs[i, j].set_ylabel(\"Imaginary\")\n",
    "        #axs[i, j].set_aspect('equal', adjustable='box')\n",
    "#plt.suptitle(\"Eigenvalues of Recurrent Weights\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model on a new test set and evaluate the accuracy\n",
    "test_xs, test_ys, _ = get_data_densematrix(500, 250, 250,\n",
    "                                             seed=1234, \n",
    "                                             mask_prob=1/2, \n",
    "                                             rank_threshold=0.2, \n",
    "                                             measurement_type=\"mask\", \n",
    "                                             include_X=False)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_test_ys = model(test_xs.to(mps_device))\n",
    "    num_correct = ((pred_test_ys > 0.5) == test_ys.to(mps_device)).sum().item()\n",
    "    test_accuracy = num_correct / 500\n",
    "print(f\"Test accuracy = {np.round(test_accuracy,6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(test_ys, pred_test_ys)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"False Positive Rate, 1 - Specificity\")\n",
    "plt.ylabel(\"True Positive Rate, Sensitivity\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialcomputation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
